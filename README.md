# IDAO

Ce sera le premier d'une s√©rie de tutoriels expliquant les bases de l'apprentissage automatique du point de vue d'un programmeur. Dans la partie 1, je montrerai comment vous pouvez int√©grer l'apprentissage automatique de base dans un projet C ++ √† l'aide de la biblioth√®que GRT .

# Qu'est-ce que l'apprentissage automatique?
L'apprentissage automatique est une approche de l'informatique qui permet aux programmes de g√©n√©rer une sortie pr√©visible bas√©e sur une entr√©e donn√©e sans utiliser de logique explicitement d√©finie.
Par exemple, en utilisant la programmation bas√©e sur la logique traditionnelle, nous pourrions √©crire une fonction qui classe les fruits, qui prend la couleur et les dimensions en entr√©e et sort le nom du fruit. Quelque chose comme √ßa:

```**C++**
string classifyFruit (Color c, Dimensions d) 
{ 
    if (c.similar ({255, 165, 0})) // green 
    { 
        if (d.similar ({10, 9, 11})) // round-ish 
        { 
             retourner "Apple"; 
        } 
        else 
        { 
             return "Pear"; 
        } 
    } 
    if (c.similar ({255, 255, 0})) // yellow 
    { 
         return "Banana"; 
    } 
    if (c.similar ({255, 165, 0})) // orange 
    { 
         return "Orange"; 
    } 
    
    retourne "Inconnu"; 
}
```

On voit que cette approche pose toutes sortes de probl√®mes. Notre fonction ne conna√Æt que quatre types de fruits, donc si nous voulons l'√©tendre pour classer √©galement les cl√©mentines, nous aurions besoin de d√©clarations suppl√©mentaires les diff√©renciant des oranges. Nous m√©langerions √©galement les poires et les pommes en fonction de la forme exacte du fruit et de la d√©finition de notre similar()m√©thode. Pour cr√©er une fonction qui classerait une large gamme de fruits avec un bon degr√© de pr√©cision, les choses deviendraient tr√®s complexes.
L'apprentissage automatique r√©sout ce probl√®me en repr√©sentant la relation entre l'entr√©e et la sortie sous forme d' √©tat plut√¥t que par le biais de r√®gles logiques. Cela signifie qu'au lieu de programmer notre classificateur √† l'aide d' if / then / elseinstructions, nous pouvons utiliser des given / thenexemples pour exprimer notre intention. Ainsi, le code pour d√©terminer le comportement de notre fonction pourrait ressembler √† ceci:
```C++
ml.given ({0, 255, 0, 10, 9, 11}) .then ("Apple"); 
ml.given ({0, 255, 0, 15, 7, 8}) .then ("Pear"); 
ml.given ({255, 255, 0, 20, 4, 4}) .then ("Banana"); 
ml.given ({255, 165, 0, 10, 10, 10}) .then ("Orange");
```
Ici, mlrepr√©sente notre objet d'apprentissage automatique, l'argument de liste pour given()repr√©senter les tuples de couleur et de dimension pour diff√©rents types de fruits et l'argument de cha√Æne pour then()repr√©senter la sortie que nous attendons du classificateur √©tant donn√© l'entr√©e.
Si la prochaine √©tape de notre programme ressemblait √† quelque chose, x = ml.classify({255, 165, 0, 10, 10, 10})nous nous attendrions √† ce que la valeur de xsoit ¬´Orange¬ª.
L'avantage de cette approche est que puisque nous avons d√©coupl√© l' √©tat de la logique , le processus de sp√©cification des relations entr√©e / sortie peut √™tre automatis√©.
Quelque chose comme:

```C++
lignes automatiques = csv.read ();
pour (auto & r: lignes) 
{ 
    ml.given (r.colour, r.dimension) .then (r.name); 
}
```

Maintenant, nous pouvons ajouter autant de types de fruits que nous le souhaitons, ou donner de nombreux exemples diff√©rents du m√™me type de fruits sans modifier notre code! La seule chose que nous devons changer est la taille et le contenu des donn√©es d'entr√©e, dans ce cas √† partir d'un fichier CSV .
Le lecteur averti va maintenant penser: n'avons-nous pas juste cr√©√© une grande table de recherche? La r√©ponse est ¬´non¬ª, car cela ne classerait que les fruits qui correspondent exactement √† la couleur et aux dimensions d'un exemple. Nous voulons plut√¥t que le syst√®me classe les nouveaux fruits qui portent le m√™me nom que l'un de nos exemples mais avec des dimensions ou des couleurs variables. Donc, x = c.classify({245, 145, 0, 11, 9, 10})nous nous attendrions √† ce que la sortie soit ¬´Orange¬ª, m√™me si la couleur et les dimensions ne correspondent pas exactement aux exemples que nous avons fournis dans nos given / thend√©clarations.
Pour y parvenir, les syst√®mes d'apprentissage automatique utilisent un ensemble de param√®tres statistiques qui d√©finissent la probabilit√© d'une sortie donn√©e en fonction des entr√©es existantes. Chaque fois qu'un nouvel exemple est fourni, ces param√®tres sont mis √† jour pour rendre le syst√®me plus pr√©cis. C'est l'essence m√™me de l'apprentissage automatique supervis√©.

# R√©capitulatif de la terminologie:

Dans l' apprentissage automatique supervis√©, nous avons un ensemble d'exemples qui d√©finissent la relation attendue entre l'entr√©e et la sortie du syst√®me. C'est ce qu'on appelle un ensemble de donn√©es .
Chaque ligne de l'ensemble de donn√©es est constitu√©e d'une √©tiquette qui d√©termine la classe de l'entr√©e (par exemple ¬´Apple¬ª) et d'un vecteur d'{245, 145, 0, 11, 9, 10} entit√©s qui d√©termine ses propri√©t√©s (par exemple, contient des entit√©s pour la couleur et les dimensions).
Notre objectif est de cr√©er une repr√©sentation de la relation statistique entre les entr√©es (vecteurs de caract√©ristiques) et les sorties (√©tiquettes de classe). C'est ce qu'on appelle un mod√®le .
Pour ce faire, nous utilisons un syst√®me pour g√©n√©rer un mod√®le √† partir de l' ensemble de donn√©es d'apprentissage et effectuer une classification √† l'aide du mod√®le. C'est ce qu'on appelle un algorithme d' apprentissage automatique .
Ainsi, un algorithme d'apprentissage automatique est cens√© ¬´apprendre¬ª √† partir d'un ensemble de donn√©es afin de g√©n√©rer un mod√®le qui peut √™tre utilis√© pour classer des vecteurs d'entit√©s en entr√©e non pr√©sents dans l'ensemble de donn√©es d'origine.

# √Ä quoi cela sert-il?

Il existe de nombreuses vari√©t√©s d'apprentissage automatique. Dans cet article, je me concentre sur la classification supervis√©e. Classification supervis√©e est utile lorsque nous voulons construire un syst√®me qui peut classer automatiquement un certain nombre de distinctsclasses d'objets. Le mot ¬´distinct¬ª est important car les algorithmes de classification fonctionnent mieux lorsque les classes d'objets ont des caract√©ristiques distinctes (couleur, forme, taille, poids, etc.) qui les s√©parent les unes des autres. La classification fonctionne moins bien si nos objets sont assez similaires et ne peuvent √™tre distingu√©s que par des variations mineures de fonctionnalit√©s. Donc, cela fonctionnerait bien pour quelque chose comme des fruits ou des visages humains (couleur des yeux, couleur des cheveux, poils du visage, forme du visage, etc.) diff√©rents, mais aurait du mal √† identifier l'emplacement d'une personne en fonction de la temp√©rature de l'air et de la couleur du ciel. Cette capacit√© √† distinguer diff√©rents objets est appel√©e s√©parabilit√© et est un concept important dans l'apprentissage automatique. Une bonne r√®gle de base est que si un humain avait du mal √† distinguer diff√©rentes classes d'objets, une machine le ferait aussi.

# Comment int√©grer l'apprentissage automatique dans mon projet C ++?

Nous avons maintenant une compr√©hension de base de l'apprentissage automatique, comment l'int√©grer dans notre projet C ++? Une bonne biblioth√®que pour cela est le Gesture Recognition Toolkit ou GRT. GRT a √©t√© d√©velopp√© pour la reconnaissance des gestes en temps r√©el et convient √† une gamme de t√¢ches d'apprentissage automatique. Il est disponible sur Windows, Mac et Linux, sous une licence MIT et peut donc √™tre utilis√© dans des projets open source ou ferm√©s. La documentation compl√®te de classe pour GRT peut √™tre trouv√©e ici .
GRT se compose d'un certain nombre de classes C ++ qui impl√©mentent chacune un algorithme d'apprentissage automatique sp√©cifique. Presque toutes les classes GRT utilisent la convention suivante:

```C++
train(...)utilise les donn√©es d'entr√©e (...)pour former un nouveau mod√®le qui peut ensuite √™tre utilis√© pour la classification.
predict(...)utilise le vecteur d'entr√©e (...)et un mod√®le entra√Æn√© pour effectuer la classification.
getPredictedClassLabel() renvoie l'√©tiquette de classe pr√©dite √† partir du vecteur d'entr√©e
save(...) enregistre un mod√®le ou un ensemble de donn√©es dans un fichier.
load(...) charge un mod√®le ou un jeu de donn√©es pr√©-form√© √† partir d'un fichier.
clear() efface un objet ML, supprimant tous les mod√®les pr√©-form√©s
```

# Immeuble GRT

Les instructions suivantes sont sp√©cifiques aux plaftorms Mac et Linux, les utilisateurs de Windows doivent se r√©f√©rer au guide de construction officiel .
Pour construire GRT, CMake est requis. CMake peut √™tre install√© √† partir de la page du projet . Sur macOS, je recommande d'utiliser Homebrew pour installer CMake.
Une fois CMake install√©, t√©l√©chargez GRT depuis le d√©p√¥t git:

```C++
$ git clone https://github.com/jamiebullock/grt
Ensuite:
$ cd grt / build 
$ mkdir tmp && cd tmp 
$ cmake .. -DBUILD_PYTHON_BINDING = OFF 
$ make
Si des erreurs de g√©n√©ration se produisent, elles peuvent √™tre signal√©es sur le suivi des probl√®mes du projet . Sinon, la version peut √™tre test√©e comme suit:
$ ./KNNExample ../../data/IrisData.grt
Cela devrait produire quelque chose de semblable au suivant:
[TRAINING KNN] Pr√©cision de l'ensemble d'entra√Ænement: 97,5
TestSample: 0 ClassLabel: 3 PredictedClassLabel: 3
TestSample: 1 ClassLabel: 2 PredictedClassLabel: 2
...
TestSample: 29 ClassLabel: 2 PredictedClassLabel: 2
Pr√©cision du test: 93,3333%
```

Ceci ex√©cute le code trouv√© dans examples/ClassificationModulesExamples/KNNExample/KNNExample.cpp- un classifieur pour les fleurs d'Iris bas√© sur les donn√©es de l' ensemble de donn√©es Iris . Pour plus d'explications, voir ici .

### Apprentissage automatique Bonjour tout le monde!
Nous sommes maintenant pr√™ts √† construire un simple classificateur de fruits en utilisant GRT!
Nous devons d'abord cr√©er un nouveau fichier source (appelons-le fruit.cpp) et inclure l'en-t√™te GRT. Il s'agit du seul en-t√™te n√©cessaire pour utiliser la plupart des fonctionnalit√©s de la biblioth√®que GRT.

```C++
#include "GRT.h" en 
utilisant l'espace de noms GRT;
typedef std :: vector <double> v_;
int main (int argc, const char * argv []) 
{ 
}
  ```
  
Ensuite, nous allons ajouter des donn√©es de notre ensemble de donn√©es de formation. Aux fins de cet exemple, nous utiliserons les m√™mes ¬´donn√©es sur les fruits¬ª en haut de cet article. Pour ce faire, nous utilisons la classe GRT ClassificationData pour cr√©er un ensemble de donn√©es simple. Au lieu de cha√Ænes, dans l'apprentissage automatique, des √©tiquettes num√©riques sont utilis√©es, nous supposons ici un mappage de: 1 = pomme, 2 = poire, 3 = banane, 4 = orange. Nous ajoutons donc √† notre main()fonction:

### Ensemble de donn√©es ClassificationData; 
```C++
dataset.setNumDimensions (6);
// Ajoutez 3 exemples pour chaque √©l√©ment pour donner √† notre classificateur suffisamment de donn√©es 
pour (int i = 0; i <3; ++ i) 
{ 
    dataset.addSample (1, v_ {0, 255, 0, 10, 9, 11} ); // Apple 
    dataset.addSample (2, v_ {0, 255, 0, 15, 7, 8}); // 
    Ensemble de donn√©es Pear.addSample (3, v_ {255, 255, 0, 20, 4, 4}); // Banana 
    dataset.addSample (4, v_ {255, 165, 0, 10, 10, 10}); // Orange 
}
```

Dans le code r√©el, nous ajouterions de nombreux autres exemples de formation afin que notre classificateur puisse bien g√©n√©raliser √† une vari√©t√© d'entr√©es. Nous utiliserions √©galement la loadDatasetFromCSVFile()m√©thode pour charger l'ensemble de donn√©es √† partir d'un fichier afin que les donn√©es puissent √™tre s√©par√©es de notre code. La documentation pour cela peut √™tre trouv√©e ici .
Ensuite, nous chargeons notre ensemble de donn√©es et formons le classificateur. Ici, nous utilisons un KNNclassificateur, qui impl√©mente l' algorithme k-NN , mais tout autre classificateur dans GRT fonctionnerait. Comme exercice, le lecteur est encourag√© √† essayer d'√©changer diff√©rents classificateurs diff√©rents de GRT √† la place de KNN.
// La classe de classification. Essayez aussi SVM! 
```C++
Classificateur KNN;
// Former notre classificateur 
classifier.train (jeu de donn√©es);        
C'est tout! Nous avons maintenant un mod√®le form√©, utilisant l'algorithme k-NN bas√© sur l'ensemble de donn√©es d'entr√©e que nous avons fourni. Maintenant, pour tester le classificateur‚Ä¶
VectorFloat testVector = v_ {0, 255, 0, 10, 9, 11};
// Pr√©diction de la sortie bas√©e sur testVector 
classifier.predict (testVector);
// R√©cup√®re l'√©tiquette 
auto classLabel = classifier. getPredictedClassLabel ();
std :: cout << "Label de classe:" << classLabel << std :: endl;
Enfin, pour d√©montrer que notre classifieur peut g√©n√©raliser √† des entr√©es non pr√©sentes dans l'ensemble de donn√©es d'origine, nous lui demanderons de classer un vecteur d'entit√© similaire, mais pas identique √† l'un de nos exemples de formation.
VectorFloat differentVector = v_ {10, 240, 40, 8, 10, 9};
classifier.predict (differentVector); 
auto otherLabel = classifier.getPredictedClassLabel ();
std :: cout << "Autre √©tiquette:" << otherLabel << std :: endl;
Puisque notre differentVectorest le plus similaire √† notre datasetvecteur pour une pomme, nous nous attendrions √† ce que le programme sorte Other label: 1. Voyons si cela fonctionne!
Liste compl√®te des codes
#include "GRT.h" en 
utilisant l'espace de noms GRT;
typedef std :: vector <double> v_;
int main (int argc, const char * argv []) 
{ 
    ClassificationData dataset; 
    dataset.setNumDimensions (6);
    // Ajoutez 3 exemples chacun pour donner √† notre classificateur suffisamment de donn√©es 
    pour (int i = 0; i <3; ++ i) 
    { 
        // Apple 
        dataset.addSample (1, v_ {0, 255, 0, 10, 9, 11 }); 
        // 
        Ensemble de donn√©es Pear.addSample (2, v_ {0, 255, 0, 15, 7, 8}); 
        // Banana 
        dataset.addSample (3, v_ {255, 255, 0, 20, 4, 4}); 
        // 
        Ensemble de donn√©es orange.addSample (4, v_ {255, 165, 0, 10, 10, 10});   
    }
    // La classe de classification. Essayez √©galement le 
    classificateur SVM KNN;
    // Former notre classificateur 
    classifier.train (jeu de donn√©es);
    // Cr√©er un vecteur de test 
    VectorFloat testVector = v_ {0, 255, 0, 10, 9, 11};
    // Pr√©diction de la sortie bas√©e sur testVector 
    classifier.predict (testVector);
    // R√©cup√®re l'√©tiquette 
    auto classLabel = classifier. getPredictedClassLabel ();
    std :: cout << "Label de classe:" << classLabel << std :: endl;
    // Essayez un vecteur d'entr√©e qui ne se trouve pas dans le jeu de donn√©es d'origine 
    VectorFloat differentVector = v_ {10, 240, 40, 8, 10, 9};
    classifier.predict (differentVector); 
    auto otherLabel = classifier.getPredictedClassLabel ();
    std :: cout << "Autre √©tiquette:" << otherLabel << std :: endl;
    retourner 0;
}
```
      
## Pour compiler le code sur macOS ou Linux, tapez ce qui suit (√† partir du m√™me tmpr√©pertoire que nous avons cr√©√© au d√©but du tutoriel).
```C++
g ++ -std = c ++ 14 fruit.cpp -ofruit -lgrt -L. -I ../../ GRT
```

Cela indique au compilateur de se lier libgrt.so dans le r√©pertoire courant, avec les en-t√™tes ../../GRT, si vous d√©placez GRT ailleurs, les arguments -Let -I devront √™tre ajust√©s.

## Enfin, nous ex√©cutons notre programme:

```C++
./fruit
```

Et la sortie devrait √™tre:
```
√âtiquette de classe: 1 
Autre √©tiquette: 1
```

Toutes nos f√©licitations! Vous venez d'√©crire un classificateur √† usage g√©n√©ral dans 26 lignes de code C ++ ü§©. Si nous chargeions notre ensemble de donn√©es √† partir d'un fichier CSV, ce serait beaucoup moins.
J'esp√®re que ce tutoriel a √©t√© utile. Restez √† l'√©coute pour plus √† l'avenir!
